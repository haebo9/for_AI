{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cRlk2WBzXAp"
      },
      "source": [
        "___\n",
        "<a href='https://honglab.ai'><p style=\"text-align:center;\"><img src='https://lh3.googleusercontent.com/lY3ySXooSmwsq5r-mRi7uiypbo0Vez6pmNoQxMFhl9fmZJkRHu5lO2vo7se_0YOzgmDyJif9fi4_z0o3ZFdwd8NVSWG6Ea80uWaf3pOHpR4GHGDV7kaFeuHR3yAjIJjDgfXMxsvw=w2400'  class=\"center\" width=\"50%\" height=\"50%\"/></p></a>\n",
        "___\n",
        "<center><em>Content Copyright by HongLab, Inc.</em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4Ww0rXxzXAq"
      },
      "source": [
        "## 대형언어모델(LLM) 바닥부터 만들기\n",
        "\n",
        "[유튜브 강의 영상 링크](https://youtu.be/osv2csoHVAo)\n",
        "\n",
        "[홍정모 연구소 디스코드 링크](https://discord.com/invite/kgR9xJkbsV)\n",
        "\n",
        "[홍정모 연구소 홈페이지 링크](https://www.honglab.ai/)\n",
        "\n",
        "#### 참고 자료\n",
        "- [Andrej Karpathy 유튜브](https://www.youtube.com/andrejkarpathy)\n",
        "- [Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)\n",
        "- [Om-Alve/smolGPT 깃헙](https://github.com/Om-Alve/smolGPT)\n",
        "- 트랜스포머 논문 - [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
        "- OpenAI GPT2 논문 - [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK2pjG3hzXAr"
      },
      "source": [
        "#### 안내사항\n",
        "\n",
        "LLM의 핵심 개념을 개인 PC에서도 간단하게 실습하면서 공부할 수 있는 학습 자료입니다. 널리 알려진 교육/학술 자료들을 참고하여 쉽게 공부할 수 있도록 요약하고 정리한 것입니다. 코딩 스타일이나 활용 범위에 대해 오해 없으시길 바랍니다.\n",
        "\n",
        "윈도우11/WSL, Python 3.9.20, Pytorch 2.6, CUDA 12.6 에서 작동을 확인하였습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7AxVGoXzXAr"
      },
      "source": [
        "#### 전체 과정 요약\n",
        "\n",
        "LLM 기반 AI 에이전트를 만들때는 핵심이 되는 LLM이 필요한데요, LLM을 바닥부터 만드는 경우 보다는 공개되어 있는 LLM 모델들을 가져다가 나의 용도에 맞도록 다듬어서 사용하는 것이 일반적입니다. 다만, 최근에는 LLM을 바닥부터 만드는 기술에 대한 진입장벽이 낮아지고 있어서 회사별로 필요한 LLM을 바닥부터 각자 만들어 사용하게 될 가능성도 높아지고 있습니다.\n",
        "\n",
        "LLM을 만들 때는\n",
        "\n",
        "1. 사전훈련(pretraining)으로 일반적인 언어 능력을 가르친 후에\n",
        "2. 미세조정(fine tuning) 단계에서 특정 업무에 적응\n",
        "\n",
        "시키는 것이 기본이 됩니다. 여기에\n",
        "\n",
        "3. 데이터베이스(+인터넷) 검색 기능을 추가\n",
        "\n",
        "하면 지식의 범위와 정확성을 높일 수 있습니다. 사람이 생각을 거듭하여 더 깊이있는 결론을 이끌어 내듯이 LLM도\n",
        "\n",
        "4. 내부적으로 질의를 반복하여 더 좋은 결론을 도출\n",
        "\n",
        "하도록 만들 수 있습니다.\n",
        "\n",
        "여기서는 LLM의 기본 원리를 이해하기 위해서 사전훈련 과정을 바닥부터 진행해보겠습니다. 훈련 과정의 큰 틀은 일반적인 머신러닝 절차를 따릅니다.\n",
        "\n",
        "1. 훈련 데이터 준비\n",
        "1. 데이터 로더 정의\n",
        "1. 모델 정의\n",
        "1. 훈련\n",
        "1. 결과 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YK6SVzgzXAr"
      },
      "source": [
        "#### 훈련 데이터 준비\n",
        "\n",
        "준비한 텍스트 파일을 읽어 들여서 정리한 후에 앞에 cleaned_가 붙은 파일 이름으로 정리합니다.\n",
        "> 예시) alice.txt &rarr; cleaned_alice.txt\n",
        "\n",
        "- 캐글 해리포터 책 - [Harry Potter Books](https://www.kaggle.com/datasets/shubhammaindola/harry-potter-books?select=02+Harry+Potter+and+the+Chamber+of+Secrets.txt)\n",
        "- 캐글 앨리스 책 - [alice.txt](https://www.kaggle.com/datasets/leelatte/alicetxt)\n",
        "- 훈련 데이터나 가중치는 제가 배포하지 않습니다. 직접 다운받거나 준비하셔야합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jk46SGmzXAr",
        "outputId": "111b0edb-78a1-4a58-f7bd-5cc9dff5e4e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.10)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pwd\n",
        "!pip install kaggle\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWEoL4y-zXAs",
        "outputId": "ee4bcb83-3aea-4009-aa53-8efd333a3b10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KAGGLE_USERNAME: habogu\n",
            "KAGGLE_KEY: 986f61db73e18e04f1c3cac004c1d0c6\n",
            "Dataset URL: https://www.kaggle.com/datasets/shubhammaindola/harry-potter-books\n",
            "License(s): CC0-1.0\n",
            "harry-potter-books.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "mkdir: harry_data: File exists\n",
            "unzip:  cannot find or open /content/harry-potter-books.zip, /content/harry-potter-books.zip.zip or /content/harry-potter-books.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# 환경 변수 사용\n",
        "kaggle_username = os.getenv(\"KAGGLE_USERNAME\")\n",
        "kaggle_key = os.getenv(\"KAGGLE_KEY\")\n",
        "\n",
        "print(f\"KAGGLE_USERNAME: {kaggle_username}\")\n",
        "print(f\"KAGGLE_KEY: {kaggle_key}\")\n",
        "\n",
        "!kaggle datasets download shubhammaindola/harry-potter-books\n",
        "!mkdir harry_data\n",
        "!unzip /content/harry-potter-books.zip -d harry_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5643be-L0sr0"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJsAPL9HzXAt",
        "outputId": "5ea475d4-09b6-4e36-e21d-7c0acde6298a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cleaned_02 Harry Potter and the Chamber of Secrets.txt 488771 characters\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "def clean_text(filename):\n",
        "    root_path = \"harry_data/\"\n",
        "    file_path = os.path.join(root_path, filename) # 원래 파일 경로 생성\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            book_text = file.read()\n",
        "\n",
        "        cleaned_text = re.sub(r'\\n+', ' ', book_text)  # 줄바꿈을 빈칸으로 변경\n",
        "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # 여러 빈칸을 하나의 빈칸으로\n",
        "\n",
        "        print(f\"cleaned_{filename}\", len(cleaned_text), \"characters\")  # 글자 수 출력\n",
        "\n",
        "        cleaned_file_path = os.path.join(root_path, f\"cleaned_{filename}\") # 정리된 파일 경로 생성\n",
        "        with open(cleaned_file_path, 'w', encoding='utf-8') as cleaned_file:\n",
        "            cleaned_file.write(cleaned_text) # 정리된 텍스트를 파일에 저장\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"파일을 찾을 수 없습니다: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"오류 발생: {e}\")\n",
        "\n",
        "filenames_list = [\"02 Harry Potter and the Chamber of Secrets.txt\"]\n",
        "\n",
        "for filename in filenames_list:\n",
        "    clean_text(filename) # filename을 인자로 전달"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfw2KFHAzXAt"
      },
      "source": [
        "#### 토큰화\n",
        "\n",
        "UTF-8 BPE(Bype Pair Encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lWX80IhzXAt",
        "outputId": "c73b9ecb-6b32-45d3-e6a8-bc2254f55a68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "글자수: 26 토큰수 6\n",
            "[18308, 14179, 373, 257, 18731, 13]\n",
            "Harry Potter was a wizard.\n",
            "18308\t -> Harry\n",
            "14179\t ->  Potter\n",
            "373\t ->  was\n",
            "257\t ->  a\n",
            "18731\t ->  wizard\n",
            "13\t -> .\n"
          ]
        }
      ],
      "source": [
        "import tiktoken # pip install tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "text = \"Harry Potter was a wizard.\"\n",
        "\n",
        "tokens = tokenizer.encode(text)\n",
        "\n",
        "print(\"글자수:\", len(text), \"토큰수\", len(tokens))\n",
        "print(tokens)\n",
        "print(tokenizer.decode(tokens))\n",
        "for t in tokens:\n",
        "    print(f\"{t}\\t -> {tokenizer.decode([t])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5lzvVRV1N_M",
        "outputId": "87b7e3e6-eea8-4c0a-a727-df71cf00e8e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 2356\n",
            "drwxr-xr-x 1 root root    4096 Feb 24 11:57 .\n",
            "drwxr-xr-x 1 root root    4096 Feb 24 11:50 ..\n",
            "drwxr-xr-x 4 root root    4096 Feb 20 14:24 .config\n",
            "drwxr-xr-x 2 root root    4096 Feb 24 12:00 harry_data\n",
            "-rw-r--r-- 1 root root 2390829 May  3  2024 harry-potter-books.zip\n",
            "drwxr-xr-x 1 root root    4096 Feb 20 14:24 sample_data\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!ls -al\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPldDmYyzXAt"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoTokenizer # pip install transformers\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct\")  # KoGPT2 사용\n",
        "# # tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\")  # KoGPT2 사용\n",
        "\n",
        "# print(\"Vocab size :\", len(tokenizer))\n",
        "\n",
        "# text = \"대사께서는 도(道)를 얻은 모양이구려.\"\n",
        "\n",
        "# tokens = tokenizer.encode(text)\n",
        "\n",
        "# print(len(text), len(tokens))\n",
        "# print(tokens)\n",
        "# print(tokenizer.decode(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkHMo8czzXAt",
        "outputId": "3248a22a-3284-470c-feae-fa893f93f96f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "H -> [39] -> H\n",
            "a -> [64] -> a\n",
            "r -> [81] -> r\n",
            "r -> [81] -> r\n",
            "y -> [88] -> y\n",
            "  -> [220] ->  \n",
            "P -> [47] -> P\n",
            "o -> [78] -> o\n",
            "t -> [83] -> t\n",
            "t -> [83] -> t\n",
            "e -> [68] -> e\n",
            "r -> [81] -> r\n",
            "  -> [220] ->  \n",
            "w -> [86] -> w\n",
            "a -> [64] -> a\n",
            "s -> [82] -> s\n",
            "  -> [220] ->  \n",
            "a -> [64] -> a\n",
            "  -> [220] ->  \n",
            "w -> [86] -> w\n",
            "i -> [72] -> i\n",
            "z -> [89] -> z\n",
            "a -> [64] -> a\n",
            "r -> [81] -> r\n",
            "d -> [67] -> d\n",
            ". -> [13] -> .\n"
          ]
        }
      ],
      "source": [
        "for char in text:\n",
        "    token_ids = tokenizer.encode(char)     # 한 글자씩 인코딩(토큰화)\n",
        "    decoded = tokenizer.decode(token_ids)  # 한 글자씩 디코딩\n",
        "    print(f\"{char} -> {token_ids} -> {decoded}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxkqpAPyzXAu"
      },
      "source": [
        "#### 데이터로더(DataLoader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjAEj291zXAu",
        "outputId": "4bfddde9-d53a-4015-8e90-ae3fca99d120"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# of tokens in txt: 130520\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, txt, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # token_ids = tokenizer.encode(\"<|endoftext|>\" + txt, allowed_special={\"<|endoftext|>\"})\n",
        "        token_ids = tokenizer.encode(txt)\n",
        "\n",
        "        print(\"# of tokens in txt:\", len(token_ids))\n",
        "\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "# with open(\"cleaned_묵향 1-36권 [전동조].txt\", 'r', encoding='utf-8-sig') as file: # 선택: -sig를 붙여서 BOM 제거\n",
        "with open(\"harry_data/cleaned_02 Harry Potter and the Chamber of Secrets.txt\", 'r', encoding='utf-8-sig') as file: # 선택: -sig를 붙여서 BOM 제거\n",
        "    txt = file.read()\n",
        "\n",
        "dataset = MyDataset(txt, max_length = 32, stride = 4)\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True)\n",
        "\n",
        "# 주의: test, valid는 생략하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IFAkaUJzXAu",
        "outputId": "26f909f9-f983-4341-8005-5c511426d2f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " my next class —” And he hurried off. “Prepare his class,” Ron sneered after him. “Gone to curl\n",
            " next class —” And he hurried off. “Prepare his class,” Ron sneered after him. “Gone to curl his\n"
          ]
        }
      ],
      "source": [
        "dataiter = iter(train_loader)\n",
        "\n",
        "x, y = next(dataiter)\n",
        "\n",
        "print(tokenizer.decode(x[0].tolist()))\n",
        "print(tokenizer.decode(y[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HOaMcstzXAu"
      },
      "source": [
        "#### 뉴럴네트워크 모델 정의\n",
        "\n",
        "모델 정의는 교재 \"[Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)\"에서 제공하는 [예제 코드](https://github.com/rasbt/LLMs-from-scratch)를 약간 수정하였습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Iyu8Yra_zXAu"
      },
      "outputs": [],
      "source": [
        "# 모델을 정의할 때 사용하는 상수들\n",
        "\n",
        "VOCAB_SIZE = tokenizer.n_vocab # 50257 Tiktoken\n",
        "#VOCAB_SIZE = len(tokenizer) # AutoTokenizer\n",
        "CONTEXT_LENGTH = 128  # Shortened context length (orig: 1024)\n",
        "EMB_DIM = 768  # Embedding dimension\n",
        "NUM_HEADS = 12  # Number of attention heads\n",
        "NUM_LAYERS = 12  # Number of layers\n",
        "DROP_RATE = 0.1  # Dropout rate\n",
        "QKV_BIAS = False  # Query-key-value bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "WrNftUiGzXAu"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out):\n",
        "        super().__init__()\n",
        "\n",
        "        assert d_out % NUM_HEADS == 0, \"d_out must be divisible by n_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.head_dim = d_out // NUM_HEADS\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=QKV_BIAS)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=QKV_BIAS)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=QKV_BIAS)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(DROP_RATE)\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(CONTEXT_LENGTH, CONTEXT_LENGTH), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x)  # (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        keys = keys.view(b, num_tokens, NUM_HEADS, self.head_dim)\n",
        "        values = values.view(b, num_tokens, NUM_HEADS, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, NUM_HEADS, self.head_dim)\n",
        "\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(2, 3)\n",
        "\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(EMB_DIM, 4 * EMB_DIM),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * EMB_DIM, EMB_DIM),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=EMB_DIM,\n",
        "            d_out=EMB_DIM)\n",
        "\n",
        "        self.ff = FeedForward()\n",
        "        self.norm1 = LayerNorm(EMB_DIM)\n",
        "        self.norm2 = LayerNorm(EMB_DIM)\n",
        "        self.drop_shortcut = nn.Dropout(DROP_RATE)\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(VOCAB_SIZE, EMB_DIM)\n",
        "        self.pos_emb = nn.Embedding(CONTEXT_LENGTH, EMB_DIM)\n",
        "        self.drop_emb = nn.Dropout(DROP_RATE)\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock() for _ in range(NUM_LAYERS)])\n",
        "\n",
        "        self.final_norm = LayerNorm(EMB_DIM)\n",
        "        self.out_head = nn.Linear(EMB_DIM, VOCAB_SIZE, bias=False)\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtMaZDFczXAu"
      },
      "source": [
        "#### 훈련"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elZ7uLKTzXAu",
        "outputId": "18956747-9cfe-403d-f24a-4178ac70bfef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = \"cpu\"\n",
        "print(device)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel()\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0asBRBOzXAv",
        "outputId": "c54c5db0-197a-4c91-ad7d-845f2ec114b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens seen: 4096\n",
            "Epoch: 1, Loss: 4.399493903625668\n",
            "Epoch: 2, Loss: 2.2281860676337413\n",
            "Epoch: 3, Loss: 0.7970672822374059\n",
            "Tokens seen: 4100096\n",
            "Epoch: 4, Loss: 0.3915339292973045\n",
            "Epoch: 5, Loss: 0.30412542092518546\n",
            "Epoch: 6, Loss: 0.27177312870429254\n",
            "Epoch: 7, Loss: 0.25313402366215787\n",
            "Tokens seen: 8196096\n",
            "Epoch: 8, Loss: 0.24359524373229094\n",
            "Epoch: 9, Loss: 0.23676939590240087\n",
            "Epoch: 10, Loss: 0.23107889570354476\n"
          ]
        }
      ],
      "source": [
        "tokens_seen, global_step = 0, -1\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for input_batch, target_batch in train_loader:\n",
        "        optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "        input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "\n",
        "        logits = model(input_batch)\n",
        "        loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "        epoch_loss += loss.item()\n",
        "        loss.backward() # Calculate loss gradients\n",
        "        optimizer.step() # Update model weights using loss gradients\n",
        "        tokens_seen += input_batch.numel()\n",
        "        global_step += 1\n",
        "\n",
        "        if global_step % 1000 == 0:\n",
        "            print(f\"Tokens seen: {tokens_seen}\")\n",
        "        # Optional evaluation step\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    losses.append(avg_loss)\n",
        "    print(f\"Epoch: {epoch + 1}, Loss: {avg_loss}\")\n",
        "    torch.save(model.state_dict(), \"model_\" + str(epoch + 1).zfill(3) + \".pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "GqbjJQQjzXAv",
        "outputId": "31245977-b058-4bcf-cc74-ac6035816db6"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVKfwCFr4XvX",
        "outputId": "a20e5a2c-1b73-4dd5-890e-71a27b83ca11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 6351396\n",
            "drwxr-xr-x 1 root root      4096 Feb 24 12:13 .\n",
            "drwxr-xr-x 1 root root      4096 Feb 24 11:50 ..\n",
            "drwxr-xr-x 4 root root      4096 Feb 20 14:24 .config\n",
            "drwxr-xr-x 2 root root      4096 Feb 24 12:00 harry_data\n",
            "-rw-r--r-- 1 root root   2390829 May  3  2024 harry-potter-books.zip\n",
            "-rw-r--r-- 1 root root 650136058 Feb 24 12:04 model_001.pth\n",
            "-rw-r--r-- 1 root root 650136058 Feb 24 12:05 model_002.pth\n",
            "-rw-r--r-- 1 root root 650136058 Feb 24 12:06 model_003.pth\n",
            "-rw-r--r-- 1 root root 650136058 Feb 24 12:07 model_004.pth\n",
            "-rw-r--r-- 1 root root 650136058 Feb 24 12:08 model_005.pth\n",
            "-rw-r--r-- 1 root root 650136058 Feb 24 12:09 model_006.pth\n",
            "-rw-r--r-- 1 root root 650136058 Feb 24 12:10 model_007.pth\n",
            "-rw-r--r-- 1 root root 650136058 Feb 24 12:11 model_008.pth\n",
            "-rw-r--r-- 1 root root 650136058 Feb 24 12:12 model_009.pth\n",
            "-rw-r--r-- 1 root root 650136058 Feb 24 12:13 model_010.pth\n",
            "drwxr-xr-x 1 root root      4096 Feb 20 14:24 sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls -al"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6oxkTmZzXAv"
      },
      "source": [
        "#### 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icAHDsJBzXAv",
        "outputId": "6984e9de-ff83-4298-9a82-e872a555ae41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(128, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 파일로 저장했던 네트워크의 가중치들 읽어들이기\n",
        "model.load_state_dict(torch.load(\"model_010.pth\", map_location=device, weights_only=True))\n",
        "model.eval() # dropout을 사용하지 않음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drmE-WKfzXAv",
        "outputId": "d3f95498-c830-454e-9576-94b5a1ef7d98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12.18\t 257\t  a\n",
            "10.22\t 991\t  still\n",
            "10.00\t 1479\t  free\n",
            "9.51\t 973\t  used\n",
            "8.74\t 11\t ,\n",
            "8.58\t 1908\t  sent\n",
            "8.48\t 3338\t  safe\n",
            "8.45\t 1464\t  always\n",
            "8.24\t 4762\t  believed\n",
            "7.96\t 8161\t  careful\n",
            " a\n"
          ]
        }
      ],
      "source": [
        "idx = tokenizer.encode(\"Dobby is\") # 토큰 id의 list\n",
        "idx = torch.tensor(idx).unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(idx)\n",
        "\n",
        "logits = logits[:, -1, :]\n",
        "\n",
        "# 가장 확률이 높은 단어 10개 출력\n",
        "top_logits, top_indices = torch.topk(logits, 10)\n",
        "for p, i in zip(top_logits.squeeze(0).tolist(), top_indices.squeeze(0).tolist()):\n",
        "    print(f\"{p:.2f}\\t {i}\\t {tokenizer.decode([i])}\")\n",
        "\n",
        "# 가장 확률이 높은 단어 출력\n",
        "idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "flat = idx_next.squeeze(0) # 배치 차원 제거 torch.Size([1])\n",
        "out = tokenizer.decode(flat.tolist()) # 텐서를 리스트로 바꿔서 디코드\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Xf4K_au_zXAv"
      },
      "outputs": [],
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        if top_k is not None:\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:\n",
        "            break\n",
        "\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyePbU46zXAv",
        "outputId": "41087323-21cb-44aa-a384-2619ef1deb6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start context: what is your\n",
            "0 : what is your best friends only, of course.” “I wouldn’t bet on that,” Harry murmured, watching Snape baring his teeth into the passage his teeth excitedly at the elf came to say anything — “…\n",
            "1 : what is your trunk we know who thinks Muggle-borns are scum?” He looked at Hermione. Hermione looked back, unconvinced. “If Lockhart’s where you’s hand on top of a trolley around\n",
            "2 : what is your trunk right away,” snapped Professor Binns. “I repeat, if the likes of Dumbledore —” “But maybe you” said if he will be dangerous, Professor McGonagall thinks he certainly kept watch you haven\n",
            "3 : what is your —” Hagrid leapt to his feet, his shaggy black head grazing the ceiling. ‘An’ how many did yeh have been she could possibly suspect him when You-borns had said he could go,’\n",
            "4 : what is your singing sorcerers —” He stuttered to a halt. Hermione’s hand was waving in the air again. “Miss Grant?’s a Draco how many students could goggily quiet.” “You could\n",
            "5 : what is your —” Hagrid leapt to his feet, his shaggy black head grazing the ceiling. ‘An’ how many did yeh have realized that girl got there was what he could possibly suspecting Club could be eating,’\n",
            "6 : what is your best friends only, of course.” “I wouldn’t bet on that,” Harry murmured, watching Snape baring his teeth into the passage his teeth thing of someone speaking at once more loudly to hold of someone sitting\n",
            "7 : what is your best friends only, of course.” “I wouldn’t bet on that,” Harry murmured, watching Snape baring his teeth into the passage his teeth. “We weren’s horrified face above him.\n",
            "8 : what is your trunk — you know — a bit o’ help —” Harry noticed Hagrid’s flowery pink umbrella leaning against the back wall of the back of the back wall of the back of the back of the back wall of the back\n",
            "9 : what is your trunk — you know — a bit o’ help —” Harry noticed Hagrid’s flowery pink umbrella leaning against the back wall of the back of the back wall of the back wall of the back wall of the back of the\n"
          ]
        }
      ],
      "source": [
        "start_context = input(\"Start context: \")\n",
        "\n",
        "# idx = tokenizer.encode(start_context, allowed_special={'<|endoftext|>'})\n",
        "idx = tokenizer.encode(start_context)\n",
        "idx = torch.tensor(idx).unsqueeze(0)\n",
        "\n",
        "context_size = model.pos_emb.weight.shape[0]\n",
        "\n",
        "for i in range(10):\n",
        "\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=idx.to(device),\n",
        "        max_new_tokens=50,\n",
        "        context_size= context_size,\n",
        "        top_k=50,\n",
        "        temperature=0.5\n",
        "    )\n",
        "\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    out = tokenizer.decode(flat.tolist()).replace(\"\\n\", \" \")\n",
        "\n",
        "    print(i, \":\", out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtI5dVwI4kfm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

import os
import re
import json
import argparse
from typing import Dict
from collections import Counter, OrderedDict

class TextPostprocessor:
    # ì´ëª¨ì§€ íŒ¨í„´ ì •ì˜
    EMOJI_PATTERN = (
        "[" +
        "\U0001F600-\U0001F64F"
        "\U0001F300-\U0001F5FF"
        "\U0001F680-\U0001F6FF"
        "\U0001F1E0-\U0001F1FF"
        "\U00002700-\U000027BF"
        "\U0001F900-\U0001F9FF"
        "\U00002600-\U000026FF"
        "]"
    )

    @staticmethod
    def clean_special_spaces(text: str) -> str:
        # íŠ¹ìˆ˜ ìœ ë‹ˆì½”ë“œ ê³µë°±ì„ ì¼ë°˜ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜
        return re.sub(r'[\u2000-\u200B\u2800\u3000]', ' ', text)

    @classmethod
    def process(cls, text: str, original_content: str = "") -> str:
        # íŠ¹ìˆ˜ ê³µë°± ì¹˜í™˜
        text = cls.clean_special_spaces(text)
        # í•´ì‹œíƒœê·¸ ì œê±°
        text = re.sub(r'#\S+', '', text)
        # ì‹¤ì œ ì¤„ë°”ê¿ˆ ë¬¸ì ì œê±°
        text = re.sub(r'(\r\n|\r|\n)', '', text)
        # ì´ìŠ¤ì¼€ì´í”„ëœ ì¤„ë°”ê¿ˆ ë¬¸ì ì œê±°
        text = re.sub(r'(\\r\\n|\\r|\\n)', '', text)
        text = re.sub(r"[ï¸â€¹â€ºï¼]", '', text)
        # ì´ëª¨ì§€ 2ê°œë§Œ ë‚¨ê¸°ê¸°
        emojis = re.findall(cls.EMOJI_PATTERN, text)
        if len(emojis) > 2:
            keep = emojis[:2]
            text = re.sub(cls.EMOJI_PATTERN, '', text) + ''.join(keep)
        # ì—°ì† íŠ¹ìˆ˜ë¬¸ì 2íšŒê¹Œì§€ë§Œ í—ˆìš©
        text = re.sub(r'([!?\.ğŸ’¢â¤â­âœ¨ğŸ¾â€¦])\1{2,}', r'\1\1', text)
        # ë°˜ë³µ ë‹¨ì–´ 2íšŒê¹Œì§€ë§Œ í—ˆìš©
        words = re.findall(r'\b\w+\b', text)
        counts = Counter(words)
        for word, count in counts.items():
            if count > 2:
                text = re.sub(rf'\b({re.escape(word)})\b', '', text, count=count - 2)
        # ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°
        for word in ['system', 'ì•ˆì˜¬ë¼ê°„ë‹¤']:
            text = text.replace(word, '')
        # ì¤‘ë³µ ë§ˆì¹¨í‘œ, ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
        text = re.sub(r'\.\.+', '.', text)
        text = re.sub(r'\s+', ' ', text).strip()
        # 5ì ë¯¸ë§Œ, ì˜ë¯¸ ì—†ëŠ” í…ìŠ¤íŠ¸ëŠ” ì˜¤ë¥˜ ë©”ì‹œì§€
        if len(text) < 5 or re.fullmatch(r'[\W\d\s]+', text):
            return "[ì¶œë ¥ ì˜¤ë¥˜] ê²°ê³¼ ìƒì„±ì´ ì‹¤íŒ¨í–ˆì–´ìš”."
        # ë³€í™˜ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸° (ìµœëŒ€ 200ì)
        if original_content:
            max_len = 200
            if len(text) > max_len:
                words = text.split()
                trimmed_text = ""
                for word in words:
                    if len(trimmed_text) + len(word) + 1 > max_len:
                        break
                    trimmed_text += word + " "
                text = trimmed_text.strip()
        return text

class DataFilter:
    @staticmethod
    def should_remove(data: Dict) -> bool:
        # content, transformed_content ëª¨ë‘ ì—†ìŒ
        if not data.get('content') and not data.get('transformed_content'):
            return True
        # content, transformed_content ëª¨ë‘ 5ì ë¯¸ë§Œì´ê±°ë‚˜ ì˜ë¯¸ ì—†ëŠ” í…ìŠ¤íŠ¸
        for key in ['content', 'transformed_content']:
            if key in data:
                text = data[key]
                if len(text) >= 5 and not re.fullmatch(r'[\W\d\s]+', text):
                    break
        else:
            return True
        
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€, ëª…ë ¹ì–´, í•´í‚¹/ê³µê²©/SQL ì¸ì ì…˜ ë“± í¬í•¨
        danger_keywords = [
            "system", "override", "drop table", "select", "union", "script", "í•´í‚¹", "attack", "hack", "sql", "delete", "insert", "update", "shutdown"
        ]
        for key in ['content', 'transformed_content']:
            if key in data:
                text = data[key].lower()
                if any(kw in text for kw in danger_keywords):
                    return True
                
        # í•œê¸€/ì˜ì–´ê°€ í•˜ë‚˜ë„ ì—†ëŠ” ê²½ìš° ì‚­ì œ
        def has_kor_eng(text: str) -> bool:
            return bool(re.search(r'[A-Za-zê°€-í£]', text))
        if not (has_kor_eng(data.get('content', '')) or has_kor_eng(data.get('transformed_content', ''))):
            return True
        
        return False

    @staticmethod
    def should_modify(data: Dict) -> bool:
        # í›„ì²˜ë¦¬(ìˆ˜ì •) ì¡°ê±´: í•´ì‹œíƒœê·¸, ì´ëª¨ì§€, ë°˜ë³µ, ë¹„ì•ŒíŒŒë²³ ì‹œì‘ ë“±
        for key in ['content', 'transformed_content']:
            if key in data:
                text = data[key]
                if (
                    DataFilter.contains_hashtags(text)
                    or DataFilter.contains_many_emojis(text)
                    or DataFilter.contains_consecutive_emojis(text)
                    or DataFilter.starts_with_non_alpha(text)
                    or DataFilter.contains_repeated_word(text)
                ):
                    return True
        if DataFilter.transformed_too_long(data):
            return True
        
        return False

    @staticmethod
    def contains_hashtags(text: str) -> bool:
        return bool(re.search(r'#\S+', text))

    @staticmethod
    def contains_many_emojis(text: str, max_emojis: int = 4) -> bool:
        emoji_re = re.compile(TextPostprocessor.EMOJI_PATTERN, flags=re.UNICODE)
        return len(re.findall(emoji_re, text)) > max_emojis

    @staticmethod
    def contains_consecutive_emojis(text: str, consecutive: int = 3) -> bool:
        pattern = re.compile(TextPostprocessor.EMOJI_PATTERN + "{" + str(consecutive) + ",}", flags=re.UNICODE)
        return bool(pattern.search(text))

    @staticmethod
    def starts_with_non_alpha(text: str) -> bool:
        text = text.strip()
        return not bool(re.match(r'^[A-Za-zê°€-í£]', text))

    @staticmethod
    def transformed_too_long(data: Dict) -> bool:
        content = data.get('content', '')
        transformed = data.get('transformed_content', '')
        if content and transformed:
            return len(transformed) > 2.0 * len(content)
        return False

    @staticmethod
    def contains_repeated_word(text: str, repeat: int = 3) -> bool:
        words = re.findall(r'\b\w+\b', text)
        word_counts = Counter(words)
        return any(count > repeat for count in word_counts.values())

def filter_and_postprocess(input_path: str, output_path: str, remove_duplicates: bool = True, skip_until_line: int = 0) -> None:
    """
    - input_path: ì…ë ¥ jsonl íŒŒì¼ ê²½ë¡œ
    - output_path: ì¶œë ¥ jsonl íŒŒì¼ ê²½ë¡œ
    - skip_until_line: í•´ë‹¹ ì¤„ê¹Œì§€ ë°ì´í„°ëŠ” ëª¨ë‘ ê±´ë„ˆëœ€(1ë¶€í„° ì‹œì‘)
    - remove_duplicates: Trueë©´ ì¤‘ë³µ ì œê±°, Falseë©´ ì¤‘ë³µ ì œê±° ì•ˆí•¨
    """
    seen = set()
    column_order = ["content", "emotion", "post_type", "transformed_content"]
    with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:
        for idx, line in enumerate(infile, 1):
            if idx <= skip_until_line:
                continue
            try:
                data = json.loads(line)
            except Exception:
                continue

            # ì¤‘ë³µ ì œê±° ê¸°ì¤€: content + emotion + post_type
            if remove_duplicates:
                key = (
                    data.get('content', '').strip(),
                    data.get('emotion', ''),
                    data.get('post_type', '')
                )
                if key in seen:
                    continue
                seen.add(key)

            # ì‚­ì œ ì¡°ê±´
            if DataFilter.should_remove(data):
                continue

            # ìˆ˜ì •(í›„ì²˜ë¦¬) ì¡°ê±´
            if DataFilter.should_modify(data):
                if 'content' in data:
                    data['content'] = TextPostprocessor.process(data['content'])
                if 'transformed_content' in data:
                    data['transformed_content'] = TextPostprocessor.process(
                        data['transformed_content'],
                        original_content=data.get('content', '')
                    )

            # ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬
            ordered = OrderedDict()
            for col in column_order:
                if col in data:
                    ordered[col] = data[col]
            for k, v in data.items():
                if k not in ordered:
                    ordered[k] = v
            data = ordered

            json.dump(data, outfile, ensure_ascii=False)
            outfile.write('\n')

def count_lines(filepath: str) -> int:
    # íŒŒì¼ì˜ ì´ ë¼ì¸ ìˆ˜ ë°˜í™˜
    return sum(1 for _ in open(filepath, encoding='utf-8'))

def count_features(output_path: str) -> None:
    """post_typeë³„ë¡œ emotion ë¶„í¬ë¥¼ í•¨ê»˜ ì¶œë ¥"""
    from collections import defaultdict

    # ì¶œë ¥í•  emotion ìˆœì„œ ì§€ì •
    emotion_order = ["normal", "happy", "sad", "grumpy", "angry", "curious"]

    # post_typeë³„ emotion ì¹´ìš´íŠ¸
    type_emotion_counter = defaultdict(lambda: Counter())
    type_total_counter = Counter()

    with open(output_path, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            post_type = data.get('post_type', 'unknown')
            emotion = data.get('emotion', 'unknown')
            type_emotion_counter[post_type][emotion] += 1
            type_total_counter[post_type] += 1

    for post_type, emotion_counter in type_emotion_counter.items():
        total = type_total_counter[post_type]
        print(f"\n--- {post_type} : ì´ {total}ê°œ ---")
        # ì§€ì •í•œ emotion ìˆœì„œëŒ€ë¡œ ì¶œë ¥
        for emotion in emotion_order:
            if emotion in emotion_counter:
                print(f"{emotion}: {emotion_counter[emotion]}")
        # ì§€ì •í•œ ìˆœì„œì— ì—†ëŠ” emotionë„ ì¶”ê°€ë¡œ ì¶œë ¥
        for emotion, count in emotion_counter.items():
            if emotion not in emotion_order:
                print(f"{emotion}: {count}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Process dataset files for finetuning.')
    parser.add_argument('-c', type=str, required=True, help='Dataset code to process')
    parser.add_argument('--skip', type=int, default=0, help='í•´ë‹¹ ì¤„ê¹Œì§€ ë°ì´í„°ëŠ” ëª¨ë‘ ì‚­ì œ(1ë¶€í„° ì‹œì‘)')
    args = parser.parse_args()

    root_path = "/Users/seo/Documents/_code/for_AI/my_project/Finetuning/dataset/"
    code = args.c
    input_file_path = os.path.join(root_path, f'_dataset/dataset_{code}_made.jsonl')
    output_file_path = os.path.join(root_path, f'_dataset/dataset_{code}_filtered.jsonl')

    # ë°ì´í„° í•„í„°ë§ ë° í›„ì²˜ë¦¬, ì§€ì •í•œ ì¤„ê¹Œì§€ ì‚­ì œ
    filter_and_postprocess(input_file_path, output_file_path, remove_duplicates = True, skip_until_line=args.skip)

    print("âœ… í•„í„°ë§ ì™„ë£Œ ë° ìƒˆë¡œìš´ íŒŒì¼ ìƒì„± ì™„ë£Œ")
    print(f"! ë³€í™˜ ì „ ë°ì´í„° ê°œìˆ˜: {count_lines(input_file_path)}")
    print(f"! ë³€í™˜ í›„ ë°ì´í„° ê°œìˆ˜: {count_lines(output_file_path)}")

    # post_type, emotionë³„ ê°œìˆ˜ ì¶œë ¥
    count_features(output_file_path)

# python3 preprocessing.py -c 0527 --skip 116
# python3 preprocessing.py -c 0613
# python3 preprocessing.py -c 0614
# python3 preprocessing.py -c 0615


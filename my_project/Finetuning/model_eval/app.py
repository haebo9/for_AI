import streamlit as st
import os, json
import tempfile
import subprocess
import pandas as pd
from io import BytesIO
from functions.visualize import load_eval_results, get_mean_scores, normalize_scores, plot_radar_chart_multi, plot_score_distribution
from functions.feature_count import get_data_distribution
from functions.filtering import filter_jsonl_bytes_by_threshold

st.title("ë§íˆ¬ë³€í™˜ ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")

# ê·œì¹™ ë³´ê¸° toggle
if st.toggle("ë°ì´í„° ì—…ë¡œë“œ ê·œì¹™ ë³´ê¸°"):
    st.markdown("""
    ### ë°ì´í„° ì—…ë¡œë“œ ê·œì¹™

    #### 1. **íŒŒì¼ í¬ë§·**
    - **í˜•ì‹:** `.jsonl` (JSON Lines, í•œ ì¤„ì— í•˜ë‚˜ì˜ JSON ê°ì²´)
    - **ì¸ì½”ë”©:** `UTF-8`

    #### 2. **í•„ìˆ˜ í•„ë“œ ë° ì˜ˆì‹œ**
    | í•„ë“œëª…                | ì„¤ëª…                        | ì˜ˆì‹œ ê°’                      |
    |-----------------------|-----------------------------|------------------------------|
    | `post_type`           | ë™ë¬¼ ìœ í˜•                   | `"cat"`, `"dog"`             |
    | `emotion`             | ê°ì •                        | `"normal"`, `"happy"`, `"sad"`, `"angry"`, `"grumpy"`, `"curious"` |
    | `content`             | ì…ë ¥ ë¬¸ì¥                   | `"ì˜¤ëŠ˜ì€ ë­˜ í•˜ê³  ë†€ê¹Œ?"`      |
    | `transformed_content` | ë³€í™˜(ì¶œë ¥) ë¬¸ì¥             | `"ë©. ì˜¤ëŠ˜ ë­í•˜ê³  ë†€ì§€? ..."`  |

    #### 3. **JSONL ì˜ˆì‹œ**
    ```json
    {
    "post_type": "dog",
    "emotion": "normal",
    "content": "ì˜¤ëŠ˜ì€ ë­˜ í•˜ê³  ë†€ê¹Œ?",
    "transformed_content": "ë©. ì˜¤ëŠ˜ ë­í•˜ê³  ë†€ì§€? ë¹¨ë¦¬ ë†€ê³  ì‹¶ë‹¤ë©. ğŸ¾"
    }
    ```

    #### 4. **í‰ê°€ ëŒ€ìƒ ë™ë¬¼/ê°ì •**
    - **ë™ë¬¼(post_type):**  
    - `cat` (ê³ ì–‘ì´)  
    - `dog` (ê°•ì•„ì§€)
    - **ê°ì •(emotion):**  
    - `normal` (ì¼ë°˜)  
    - `happy` (ê¸°ì¨)  
    - `sad` (ìŠ¬í””)  
    - `angry` (ë¶„ë…¸)  
    - `grumpy` (ê¹Œì¹ )  
    - `curious` (í˜¸ê¸°ì‹¬)

    #### 5. **ì—…ë¡œë“œ ì‹œ ì£¼ì˜ì‚¬í•­**
    - ê° ì¤„ë§ˆë‹¤ í•˜ë‚˜ì˜ JSON ê°ì²´ë§Œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
    - ëª¨ë“  í•„ë“œëŠ” ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•˜ë©°, ê°’ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ì…ë ¥í•˜ì„¸ìš”.
    - íŒŒì¼ í¬ê¸°ëŠ” 10MB ì´í•˜ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.
    - í‰ê°€ ì ìˆ˜(`kobert_score`, `type_score`, `quality_score`, `bleu_score`, `perplexity_score` ë“±)ëŠ” í‰ê°€ê°€ ëë‚œ íŒŒì¼ì—ë§Œ í¬í•¨ë©ë‹ˆë‹¤.

    #### 6. **í‰ê°€ ê¸°ì¤€ ë° ì˜ë¯¸**
    | ì ìˆ˜ëª…              | ì˜ë¯¸                                                             | íŠ¹ì§•     |
    |-------------------|-----------------------------------------------------------------|---------|
    | kobert_score      | KoBERT ê¸°ë°˜ ì˜ë¯¸ ìœ ì‚¬ë„ (ì›ë¬¸ê³¼ ë³€í™˜ë¬¸ì¥ì˜ ì˜ë¯¸ê°€ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ í‰ê°€)        | 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì˜ë¯¸ê°€ ìœ ì‚¬í•¨ (ìµœëŒ€ 1) |
    | type_score        | ë§íˆ¬/ìŠ¤íƒ€ì¼ ì í•©ì„± (ë™ë¬¼/ê°ì •ë³„ ìš”êµ¬ ì¡°ê±´ì— ë§ëŠ”ì§€ í‰ê°€)                     | 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë§íˆ¬/ìŠ¤íƒ€ì¼ì´ ì í•© (ìµœëŒ€ 1) |
    | quality_score     | í’ˆì§ˆ(ì˜ë¯¸ ë³´ì¡´, ìŠ¤íƒ€ì¼ ì¼ì¹˜, ìì—°ìŠ¤ëŸ¬ì›€, í˜•ì‹ ì í•©ì„± ë“± ë³€í™˜ë¬¸ì¥ í’ˆì§ˆ í‰ê°€)      | 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ í’ˆì§ˆì´ ìš°ìˆ˜ (ìµœëŒ€ 1) |
    | bleu_score        | BLEU ì ìˆ˜ (ì›ë¬¸ê³¼ ë³€í™˜ë¬¸ì¥ ê°„ n-gram ê¸°ë°˜ ìœ ì‚¬ë„, ê¸°ê³„ë²ˆì—­ í’ˆì§ˆ ì§€í‘œ)        | `bleu_score / 0.1` ë¡œ 10ë°° ê°’ ì‚¬ìš©(ìµœëŒ€ 1) |
    | perplexity_score  | ë¬¸ì¥ ìì—°ìŠ¤ëŸ¬ì›€ (ì–¸ì–´ëª¨ë¸ ê¸°ë°˜ Perplexity, ë‚®ì„ìˆ˜ë¡ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥)          | `0.1 / perplexity_score` ë¡œ ì—­ìˆ˜ ë° 1/10ê°’ ì‚¬ìš©(ìµœëŒ€ 1) |

    - **ëª¨ë“  ì ìˆ˜ëŠ” 0~1ë¡œ ë³€í™˜ë˜ì–´ ì‹œê°í™” ë° í•„í„°ë§ì— ì‚¬ìš©ë©ë‹ˆë‹¤.**

    ---
    **ì˜ˆì‹œ íŒŒì¼ì„ ì°¸ê³ í•˜ì—¬ ë™ì¼í•œ êµ¬ì¡°ë¡œ ë°ì´í„°ë¥¼ ì¤€ë¹„í•´ ì£¼ì„¸ìš”.**
    ë¬¸ì œê°€ ìˆìœ¼ë©´ ë‹´ë‹¹ìì—ê²Œ ë¬¸ì˜ ë°”ëë‹ˆë‹¤.
        """)
    
metric_labels = {
    "kobertscore_f1": "KoBERT",
    "type_score": "Type",
    "quality_score": "Quality",
    "bleu_score": "BLEU",
    "perplexity_score": "Perplexity"
}

all_metrics = list(metric_labels.keys())

# --- ì—…ë¡œë“œ íŒŒì¼ ë° í‰ê°€ ê²°ê³¼ ìºì‹± ---
if "cached_files" not in st.session_state:
    # {íŒŒì¼ëª…: {"data": bytes, "eval": eval_jsonl_bytes, "mean_scores": dict, "dist": dict}}
    st.session_state["cached_files"] = {}

uploaded_files = st.file_uploader(
    "ì—¬ëŸ¬ ê°œì˜ JSONL ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”. (ë¶„í¬ í†µê³„ ë° ëª¨ë¸ í‰ê°€ ìë™ ì§„í–‰)",
    type=["jsonl"],
    accept_multiple_files=True,
    key="data_and_eval"
)

if not uploaded_files:
    sample_files = ["Sample.jsonl"]
    uploaded_files = []
    for path in sample_files:
        with open(path, "rb") as f:
            # Streamlitì˜ UploadedFileê³¼ ìœ ì‚¬í•œ ê°ì²´ë¡œ ë˜í•‘ í•„ìš”
            from io import BytesIO
            class DummyFile:
                def __init__(self, name, data):
                    self.name = name
                    self._data = data
                def getvalue(self):
                    return self._data
            uploaded_files.append(DummyFile(path, f.read()))

# ìƒˆë¡œ ì—…ë¡œë“œëœ íŒŒì¼ì„ ìºì‹œì— ì €ì¥ ë° í‰ê°€/í†µê³„ ìˆ˜í–‰
if uploaded_files:
    for f in uploaded_files:
        fname = f.name
        if fname not in st.session_state["cached_files"]:
            # 1. íŒŒì¼ ì €ì¥
            st.session_state["cached_files"][fname] = {"data": f.getvalue(), "eval": None, "mean_scores": None, "dist": None}
            # 2. ë°ì´í„° ë¶„í¬ í†µê³„
            dist = get_data_distribution(f.getvalue())
            st.session_state["cached_files"][fname]["dist"] = dist
            # 3. ëª¨ë¸ í‰ê°€ (í‰ê· ê°’ë§Œ ì €ì¥)
            with tempfile.NamedTemporaryFile(delete=False, suffix=".jsonl") as tmp_file:
                tmp_file.write(f.getvalue())
                tmp_path = tmp_file.name
            eval_path = tmp_path.replace(".jsonl", "_eval.jsonl")
            main_eval_path = os.path.join(os.path.dirname(__file__), "functions", "main_eval.py")
            with st.spinner(f"ëª¨ë¸ í‰ê°€ ìˆ˜í–‰ ì¤‘: {fname}"):
                result = subprocess.run(
                    [
                        "python", main_eval_path,
                        "--input_path", tmp_path,
                        "--output_path", eval_path,
                        "--use_kobert", "--use_type", "--use_quality", "--use_bleu", "--use_perplexity"
                    ],
                    capture_output=True, text=True
                )
            if result.returncode != 0:
                st.error(f"í‰ê°€ ì‹¤íŒ¨: {fname}\n{result.stderr}")
                continue
            with open(eval_path, "rb") as f_eval:
                eval_jsonl_bytes = f_eval.read()
                st.session_state["cached_files"][fname]["eval"] = eval_jsonl_bytes
            results = load_eval_results(eval_path)
            mean_scores = get_mean_scores(results, all_metrics)
            mean_scores = normalize_scores(mean_scores)
            st.session_state["cached_files"][fname]["mean_scores"] = mean_scores
            st.success(f"âœ… {fname} í‰ê°€ ë° í†µê³„ ì™„ë£Œ!")


# ìºì‹œì— ì €ì¥ëœ íŒŒì¼ ëª©ë¡
cached_file_names = list(st.session_state["cached_files"].keys())
selected_cached_files = st.multiselect(
    "ìºì‹œì— ì €ì¥ëœ íŒŒì¼ì„ ì„ íƒí•´ì„œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    cached_file_names,
    default=cached_file_names
)

# --- í‰ê°€ ê²°ê³¼ ì „ì²´(jsonl) ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ ì¶”ê°€ ---
st.markdown("#### í‰ê°€ ê²°ê³¼ íŒŒì¼(jsonl) ë‹¤ìš´ë¡œë“œ")
download_file = st.selectbox(
    "ë‹¤ìš´ë¡œë“œí•  í‰ê°€ ê²°ê³¼ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.",
    cached_file_names
)
if download_file:
    eval_bytes = st.session_state["cached_files"][download_file].get("eval")
    if eval_bytes is not None:
        st.download_button(
            label=f"ë‹¤ìš´ë¡œë“œ",
            data=eval_bytes,
            file_name=f"{os.path.splitext(download_file)[0]}_eval.jsonl",
            mime="application/json"
        )
    else:
        st.warning("ë‹¤ìš´ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ì‹¤ì œ ì‚¬ìš©í•  íŒŒì¼ ë¦¬ìŠ¤íŠ¸ (ìºì‹œì—ì„œ ì„ íƒ)
file_objs = []
for fname in selected_cached_files:
    file_obj = BytesIO(st.session_state["cached_files"][fname]["data"])
    file_obj.name = fname
    file_objs.append(file_obj)

scores_list = []
model_names = []
table_rows = []

if file_objs:
    st.markdown("-----------------------")
    for uploaded_file in file_objs:
        fname = uploaded_file.name
        dist = st.session_state["cached_files"][fname]["dist"]
        st.markdown(f"#### â¬‡ï¸ {fname} ë°ì´í„° ë¶„í¬ (ë™ë¬¼ë³„/ê°ì •ë³„)")
        # í”¼ë²— í…Œì´ë¸” ìƒì„±
        pivot_data = []
        for post_type, emotion_counter in dist["type_emotion_counter"].items():
            row = {"post_type": post_type}
            for emotion in dist["emotion_order"]:
                row[emotion] = emotion_counter.get(emotion, 0)
            for emotion, count in emotion_counter.items():
                if emotion not in dist["emotion_order"]:
                    row[emotion] = count
            pivot_data.append(row)
        pivot_df = pd.DataFrame(pivot_data).set_index("post_type")
        st.dataframe(pivot_df, use_container_width=True)
        st.success(f"âœ… ì´ ë°ì´í„° ê°œìˆ˜: {dist['total_count']} / ì¤‘ë³µ ì—†ëŠ” ì›ë¬¸ ê°œìˆ˜: {dist['unique_content_count']}")

        model_name = os.path.splitext(fname)[0]
        model_names.append(model_name)
        mean_scores = st.session_state["cached_files"][fname]["mean_scores"]
        table_row = {"ëª¨ë¸ëª…": model_name}
        table_row.update({metric_labels[metric]: mean_scores.get(metric, None) for metric in all_metrics})
        table_rows.append(table_row)
        scores_list.append(mean_scores)

    if table_rows:
        st.markdown("-----------------------")
        st.markdown("#### ì‹œê°í™”í•  í‰ê°€ ì§€í‘œ ë° threshold ê°’ì„ ì„ íƒí•˜ì„¸ìš”.")

        selected_metrics = []
        thresholds = {}
        cols = st.columns(len(all_metrics))

        # ê° ì§€í‘œë³„ thresholdì˜ ì´ˆê¸°ê°’ì„ ë¯¸ë¦¬ ì§€ì •
        default_thresholds = {
            "kobertscore_f1": 0.6,
            "type_score": 0.7,
            "quality_score": 0.7,
            "bleu_score": 0.2,
            "perplexity_score": 0.2
        }
        for i, metric in enumerate(all_metrics):
            label = metric_labels[metric]
            with cols[i]:
                checked = st.session_state.get(f"metric_{metric}", True)
                if st.checkbox(label, value=checked, key=f"metric_{metric}"):
                    selected_metrics.append(metric)
                    thresholds[metric] = st.number_input(
                        f"{label} threshold", min_value=0.0, max_value=1.0,
                        value=default_thresholds.get(metric, 0.5), step=0.01, key=f"thres_{metric}"
                    )

        if scores_list and selected_metrics:
            st.markdown("#### ì„ íƒí•œ ì§€í‘œë¡œ ë ˆì´ë” ì°¨íŠ¸ ì‹œê°í™”")
            plot_radar_chart_multi(
                scores_list, model_names, selected_metrics,
                title="Evaluation Score", metric_labels=metric_labels, thresholds=thresholds
            )

            st.markdown("#### ì„ íƒí•œ ì§€í‘œë³„ ì ìˆ˜ ë¶„í¬ (íˆìŠ¤í† ê·¸ë¨ & ë°•ìŠ¤í”Œë¡¯)")
            eval_jsonl_bytes_list = [
                st.session_state["cached_files"][fname]["eval"] for fname in selected_cached_files
            ]
            plot_score_distribution(
                eval_jsonl_bytes_list,
                [os.path.splitext(fname)[0] for fname in selected_cached_files],
                selected_metrics,
                metric_labels=metric_labels,
                thresholds=thresholds
            )

            st.markdown("-----------------------")
            st.markdown("#### ì„ íƒëœ ë°ì´í„° ì „ì²´ë¥¼ í•©ì³ì„œ í•„í„°ë§ ë° ë‹¤ìš´ë¡œë“œ")
            if selected_cached_files:
                st.markdown("**ì„ íƒëœ ë°ì´í„°ì…‹:** " + ", ".join(selected_cached_files))
            else:
                st.markdown("**ì„ íƒëœ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.**")

            # ì ìš©ëœ threshold ê°’ë§Œ ì¶œë ¥
            thres_str = ", ".join([f"{metric_labels[m]}: {thresholds[m]}" for m in thresholds])
            st.success(f"âœ… ì ìš©ëœ threshold ê°’: {thres_str}")

            eval_jsonl_bytes_list = [
                st.session_state["cached_files"][fname]["eval"] for fname in selected_cached_files
            ]

            if st.button("í•„í„°ë§"):
                total_before = sum(
                    len(st.session_state["cached_files"][fname]["eval"].decode("utf-8").splitlines())
                    for fname in selected_cached_files
                )

                filtered_data = filter_jsonl_bytes_by_threshold(eval_jsonl_bytes_list, thresholds)
                total_after = len(filtered_data)

                score_keys = set(metric_labels.keys())
                filtered_data_no_scores = []
                for d in filtered_data:
                    filtered = {k: v for k, v in d.items() if k not in score_keys}
                    filtered_data_no_scores.append(filtered)

                filtered_jsonl = "\n".join([json.dumps(d, ensure_ascii=False) for d in filtered_data_no_scores])

                st.write(f"í•„í„°ë§ ì „ ë°ì´í„° ê°œìˆ˜: {total_before} â†’ í•„í„°ë§ í›„ ë°ì´í„° ê°œìˆ˜: {total_after} (ê°ì†Œ: {total_before - total_after})")

                st.markdown("#### í•„í„°ë§ëœ ë°ì´í„°ì…‹ ë¶„í¬ í†µê³„ (í…Œì´ë¸”)")
                from functions.feature_count import get_data_distribution
                filtered_jsonl_bytes = filtered_jsonl.encode("utf-8")
                dist = get_data_distribution(filtered_jsonl_bytes)
                st.success(f"âœ… ì´ ë°ì´í„° ê°œìˆ˜: {dist['total_count']} / ì¤‘ë³µ ì—†ëŠ” ì›ë¬¸ ê°œìˆ˜: {dist['unique_content_count']}")

                pivot_data = []
                for post_type, emotion_counter in dist["type_emotion_counter"].items():
                    row = {"post_type": post_type}
                    for emotion in dist["emotion_order"]:
                        row[emotion] = emotion_counter.get(emotion, 0)
                    for emotion, count in emotion_counter.items():
                        if emotion not in dist["emotion_order"]:
                            row[emotion] = count
                    pivot_data.append(row)
                pivot_df = pd.DataFrame(pivot_data).set_index("post_type")
                st.dataframe(pivot_df, use_container_width=True)

                st.download_button(
                    label="ë‹¤ìš´ë¡œë“œ",
                    data=filtered_jsonl.encode("utf-8"),
                    file_name="filtered_all.jsonl",
                    mime="application/json"
                )
else:
    st.info("JSONL íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜, ìºì‹œì—ì„œ íŒŒì¼ì„ ì„ íƒí•˜ë©´ ëª¨ë¸ í‰ê°€ì™€ ë°ì´í„° ë¶„í¬ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")